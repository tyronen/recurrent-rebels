{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd42943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet sqlalchemy psycopg2-binary pandas tqdm pyarrow ipywidgets scikit-learn\n",
    "!jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10386e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "from sqlalchemy import text\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef7e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Do not duplicate code for this class\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        # TODO Do we have to do max norm here?\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, max_norm=1.0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bb433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"cbow_final_with_vocab.pt\", map_location=torch.device('cpu'))\n",
    "\n",
    "word2idx = checkpoint['word2idx']\n",
    "idx2word = checkpoint['idx2word']\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 100\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5323f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"sy91dhb\"\n",
    "password = \"g5t49ao\"\n",
    "host = \"178.156.142.230\"\n",
    "port = \"5432\"\n",
    "db = \"hd64m1ki\"\n",
    "\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b749c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT title, url, by, time, score FROM hacker_news.items WHERE type='story' AND dead IS NULL\"\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"hackernews_stories.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"hackernews_stories.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fffdee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.score >= 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a49f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score_log'] = df['score'].apply(lambda x: math.log(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleWordIndicesDataset(Dataset):\n",
    "    def __init__(self, dataframe, word2idx):\n",
    "        self.samples = []\n",
    "        for _, row in tqdm(dataframe.iterrows(), total=len(dataframe), desc=\"Processing titles\"):\n",
    "            title = row['title']\n",
    "            score_val = row['score_log']\n",
    "            if not isinstance(title, str) or pd.isna(score_val):\n",
    "                continue\n",
    "            score = torch.tensor(score_val, dtype=torch.float32)\n",
    "            words = title.lower().split()\n",
    "            indices = [word2idx[w] for w in words if w in word2idx]\n",
    "            if indices:\n",
    "                self.samples.append((indices, score))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d8329a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e89efee174f4ae79bb169af7ee953c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing titles:   0%|          | 0/3256354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299d9c9d74404c36b21cb266ac1fe3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing titles:   0%|          | 0/814089 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort the dataframe by time\n",
    "df_sorted = df.sort_values('time')\n",
    "\n",
    "# Calculate the split index\n",
    "split_idx = int(len(df_sorted) * 0.8)\n",
    "\n",
    "# Split into train and test\n",
    "df_train = df_sorted.iloc[:split_idx].reset_index(drop=True)\n",
    "df_test = df_sorted.iloc[split_idx:].reset_index(drop=True)\n",
    "\n",
    "# Create datasets\n",
    "title_indices_train = TitleWordIndicesDataset(df_train, word2idx)\n",
    "title_indices_test = TitleWordIndicesDataset(df_test, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a0d4086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 1.5576210926494947\n"
     ]
    }
   ],
   "source": [
    "average_score = df['score_log'].mean()\n",
    "print(f\"Average score: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55425f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleRegressionNN(nn.Module):\n",
    "    def __init__(self, embedding_layer, embedding_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        # Use the pretrained embedding weights from the CBOW model\n",
    "        self.embeddings = embedding_layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output is a single numeric value\n",
    "\n",
    "    def forward(self, input_indices):\n",
    "        # input_indices: (batch_size, seq_len)\n",
    "        embeds = self.embeddings(input_indices)  # (batch_size, seq_len, embedding_dim)\n",
    "        avg_embeds = embeds.mean(dim=1)         # (batch_size, embedding_dim)\n",
    "        x = self.fc1(avg_embeds)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(-1)  # (batch_size,)\n",
    "\n",
    "model_reg = TitleRegressionNN(model.embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09883bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd07ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/25252], Loss: 3853.3601\n",
      "Epoch [1/3], Step [200/25252], Loss: 3462.7609\n",
      "Epoch [1/3], Step [300/25252], Loss: 3211.3832\n",
      "Epoch [1/3], Step [400/25252], Loss: 3677.0475\n",
      "Epoch [1/3], Step [500/25252], Loss: 3591.5478\n",
      "Epoch [1/3], Step [600/25252], Loss: 3307.8446\n",
      "Epoch [1/3], Step [700/25252], Loss: 4321.3422\n",
      "Epoch [1/3], Step [800/25252], Loss: 3307.4646\n",
      "Epoch [1/3], Step [900/25252], Loss: 3335.7904\n",
      "Epoch [1/3], Step [1000/25252], Loss: 3269.4956\n",
      "Epoch [1/3], Step [1100/25252], Loss: 3195.2012\n",
      "Epoch [1/3], Step [1200/25252], Loss: 3826.9996\n",
      "Epoch [1/3], Step [1300/25252], Loss: 3981.0746\n",
      "Epoch [1/3], Step [1400/25252], Loss: 4227.4546\n",
      "Epoch [1/3], Step [1500/25252], Loss: 3346.5518\n",
      "Epoch [1/3], Step [1600/25252], Loss: 3162.9413\n",
      "Epoch [1/3], Step [1700/25252], Loss: 3678.6170\n",
      "Epoch [1/3], Step [1800/25252], Loss: 3114.2157\n",
      "Epoch [1/3], Step [1900/25252], Loss: 3955.8900\n",
      "Epoch [1/3], Step [2000/25252], Loss: 6528.0529\n",
      "Epoch [1/3], Step [2100/25252], Loss: 3312.9487\n",
      "Epoch [1/3], Step [2200/25252], Loss: 3966.9008\n",
      "Epoch [1/3], Step [2300/25252], Loss: 3248.6036\n",
      "Epoch [1/3], Step [2400/25252], Loss: 3609.8434\n",
      "Epoch [1/3], Step [2500/25252], Loss: 2589.8792\n",
      "Epoch [1/3], Step [2600/25252], Loss: 3356.4766\n",
      "Epoch [1/3], Step [2700/25252], Loss: 2869.7627\n",
      "Epoch [1/3], Step [2800/25252], Loss: 3501.4994\n",
      "Epoch [1/3], Step [2900/25252], Loss: 3135.3957\n",
      "Epoch [1/3], Step [3000/25252], Loss: 4521.2762\n",
      "Epoch [1/3], Step [3100/25252], Loss: 3485.4543\n",
      "Epoch [1/3], Step [3200/25252], Loss: 4515.1247\n",
      "Epoch [1/3], Step [3300/25252], Loss: 3652.1408\n",
      "Epoch [1/3], Step [3400/25252], Loss: 4322.6036\n",
      "Epoch [1/3], Step [3500/25252], Loss: 3125.1543\n",
      "Epoch [1/3], Step [3600/25252], Loss: 3754.1011\n",
      "Epoch [1/3], Step [3700/25252], Loss: 3086.7419\n",
      "Epoch [1/3], Step [3800/25252], Loss: 4500.6575\n",
      "Epoch [1/3], Step [3900/25252], Loss: 4021.5965\n",
      "Epoch [1/3], Step [4000/25252], Loss: 3301.2475\n",
      "Epoch [1/3], Step [4100/25252], Loss: 3676.5022\n",
      "Epoch [1/3], Step [4200/25252], Loss: 3540.1779\n",
      "Epoch [1/3], Step [4300/25252], Loss: 5210.3337\n",
      "Epoch [1/3], Step [4400/25252], Loss: 2830.5431\n",
      "Epoch [1/3], Step [4500/25252], Loss: 3523.7844\n",
      "Epoch [1/3], Step [4600/25252], Loss: 4274.4010\n",
      "Epoch [1/3], Step [4700/25252], Loss: 3630.8230\n",
      "Epoch [1/3], Step [4800/25252], Loss: 2824.6730\n",
      "Epoch [1/3], Step [4900/25252], Loss: 3288.0511\n",
      "Epoch [1/3], Step [5000/25252], Loss: 3307.3945\n",
      "Epoch [1/3], Step [5100/25252], Loss: 3770.2578\n",
      "Epoch [1/3], Step [5200/25252], Loss: 3292.1137\n",
      "Epoch [1/3], Step [5300/25252], Loss: 3689.6775\n",
      "Epoch [1/3], Step [5400/25252], Loss: 3524.2857\n",
      "Epoch [1/3], Step [5500/25252], Loss: 3933.4650\n",
      "Epoch [1/3], Step [5600/25252], Loss: 4174.8065\n",
      "Epoch [1/3], Step [5700/25252], Loss: 4666.5411\n",
      "Epoch [1/3], Step [5800/25252], Loss: 3517.0989\n",
      "Epoch [1/3], Step [5900/25252], Loss: 3437.0692\n",
      "Epoch [1/3], Step [6000/25252], Loss: 3600.9916\n",
      "Epoch [1/3], Step [6100/25252], Loss: 3754.6202\n",
      "Epoch [1/3], Step [6200/25252], Loss: 3508.6818\n",
      "Epoch [1/3], Step [6300/25252], Loss: 4192.4569\n",
      "Epoch [1/3], Step [6400/25252], Loss: 3506.6087\n",
      "Epoch [1/3], Step [6500/25252], Loss: 3424.8058\n",
      "Epoch [1/3], Step [6600/25252], Loss: 4215.1838\n",
      "Epoch [1/3], Step [6700/25252], Loss: 4856.9787\n",
      "Epoch [1/3], Step [6800/25252], Loss: 3675.7202\n",
      "Epoch [1/3], Step [6900/25252], Loss: 2739.0516\n",
      "Epoch [1/3], Step [7000/25252], Loss: 3628.3268\n",
      "Epoch [1/3], Step [7100/25252], Loss: 4197.8355\n",
      "Epoch [1/3], Step [7200/25252], Loss: 3762.0874\n",
      "Epoch [1/3], Step [7300/25252], Loss: 3146.0267\n",
      "Epoch [1/3], Step [7400/25252], Loss: 3343.9195\n",
      "Epoch [1/3], Step [7500/25252], Loss: 3361.3654\n",
      "Epoch [1/3], Step [7600/25252], Loss: 3208.6530\n",
      "Epoch [1/3], Step [7700/25252], Loss: 3671.5697\n",
      "Epoch [1/3], Step [7800/25252], Loss: 3576.7865\n",
      "Epoch [1/3], Step [7900/25252], Loss: 3766.1470\n",
      "Epoch [1/3], Step [8000/25252], Loss: 3316.4376\n",
      "Epoch [1/3], Step [8100/25252], Loss: 3119.7349\n",
      "Epoch [1/3], Step [8200/25252], Loss: 5062.4260\n",
      "Epoch [1/3], Step [8300/25252], Loss: 3329.4366\n",
      "Epoch [1/3], Step [8400/25252], Loss: 3637.5505\n",
      "Epoch [1/3], Step [8500/25252], Loss: 4509.2950\n",
      "Epoch [1/3], Step [8600/25252], Loss: 3540.3146\n",
      "Epoch [1/3], Step [8700/25252], Loss: 4650.1655\n",
      "Epoch [1/3], Step [8800/25252], Loss: 4186.7667\n",
      "Epoch [1/3], Step [8900/25252], Loss: 2930.7935\n",
      "Epoch [1/3], Step [9000/25252], Loss: 3655.3411\n",
      "Epoch [1/3], Step [9100/25252], Loss: 3359.2488\n",
      "Epoch [1/3], Step [9200/25252], Loss: 3055.8058\n",
      "Epoch [1/3], Step [9300/25252], Loss: 3498.9600\n",
      "Epoch [1/3], Step [9400/25252], Loss: 3375.5459\n",
      "Epoch [1/3], Step [9500/25252], Loss: 3499.6738\n",
      "Epoch [1/3], Step [9600/25252], Loss: 5230.5918\n",
      "Epoch [1/3], Step [9700/25252], Loss: 2886.9071\n",
      "Epoch [1/3], Step [9800/25252], Loss: 3460.8068\n",
      "Epoch [1/3], Step [9900/25252], Loss: 3676.3236\n",
      "Epoch [1/3], Step [10000/25252], Loss: 3683.4276\n",
      "Epoch [1/3], Step [10100/25252], Loss: 3384.6933\n",
      "Epoch [1/3], Step [10200/25252], Loss: 3953.3805\n",
      "Epoch [1/3], Step [10300/25252], Loss: 3464.7779\n",
      "Epoch [1/3], Step [10400/25252], Loss: 5072.7235\n",
      "Epoch [1/3], Step [10500/25252], Loss: 3298.1627\n",
      "Epoch [1/3], Step [10600/25252], Loss: 3017.8264\n",
      "Epoch [1/3], Step [10700/25252], Loss: 4119.5356\n",
      "Epoch [1/3], Step [10800/25252], Loss: 3738.1268\n",
      "Epoch [1/3], Step [10900/25252], Loss: 4597.0452\n",
      "Epoch [1/3], Step [11000/25252], Loss: 3419.1823\n",
      "Epoch [1/3], Step [11100/25252], Loss: 3526.4239\n",
      "Epoch [1/3], Step [11200/25252], Loss: 2959.7088\n",
      "Epoch [1/3], Step [11300/25252], Loss: 3586.9409\n",
      "Epoch [1/3], Step [11400/25252], Loss: 3249.6135\n",
      "Epoch [1/3], Step [11500/25252], Loss: 3254.8042\n",
      "Epoch [1/3], Step [11600/25252], Loss: 3510.1206\n",
      "Epoch [1/3], Step [11700/25252], Loss: 3717.9805\n",
      "Epoch [1/3], Step [11800/25252], Loss: 3796.6716\n",
      "Epoch [1/3], Step [11900/25252], Loss: 3552.0160\n",
      "Epoch [1/3], Step [12000/25252], Loss: 3602.2150\n",
      "Epoch [1/3], Step [12100/25252], Loss: 3502.1765\n",
      "Epoch [1/3], Step [12200/25252], Loss: 5883.7288\n",
      "Epoch [1/3], Step [12300/25252], Loss: 3180.4323\n",
      "Epoch [1/3], Step [12400/25252], Loss: 4298.0842\n",
      "Epoch [1/3], Step [12500/25252], Loss: 4127.0915\n",
      "Epoch [1/3], Step [12600/25252], Loss: 3683.6835\n",
      "Epoch [1/3], Step [12700/25252], Loss: 4770.0543\n",
      "Epoch [1/3], Step [12800/25252], Loss: 4643.7359\n",
      "Epoch [1/3], Step [12900/25252], Loss: 2982.2656\n",
      "Epoch [1/3], Step [13000/25252], Loss: 3666.9945\n",
      "Epoch [1/3], Step [13100/25252], Loss: 3244.3819\n",
      "Epoch [1/3], Step [13200/25252], Loss: 4635.3278\n",
      "Epoch [1/3], Step [13300/25252], Loss: 2981.9804\n",
      "Epoch [1/3], Step [13400/25252], Loss: 3467.5239\n",
      "Epoch [1/3], Step [13500/25252], Loss: 3333.2985\n",
      "Epoch [1/3], Step [13600/25252], Loss: 3884.8553\n",
      "Epoch [1/3], Step [13700/25252], Loss: 3376.9366\n",
      "Epoch [1/3], Step [13800/25252], Loss: 3374.1615\n",
      "Epoch [1/3], Step [13900/25252], Loss: 3385.5236\n",
      "Epoch [1/3], Step [14000/25252], Loss: 3694.0996\n",
      "Epoch [1/3], Step [14100/25252], Loss: 2817.7629\n",
      "Epoch [1/3], Step [14200/25252], Loss: 4411.0982\n",
      "Epoch [1/3], Step [14300/25252], Loss: 4371.8782\n",
      "Epoch [1/3], Step [14400/25252], Loss: 3191.1648\n",
      "Epoch [1/3], Step [14500/25252], Loss: 3394.0071\n",
      "Epoch [1/3], Step [14600/25252], Loss: 3734.2571\n",
      "Epoch [1/3], Step [14700/25252], Loss: 2707.1233\n",
      "Epoch [1/3], Step [14800/25252], Loss: 3216.8775\n",
      "Epoch [1/3], Step [14900/25252], Loss: 4080.7837\n",
      "Epoch [1/3], Step [15000/25252], Loss: 4142.6783\n",
      "Epoch [1/3], Step [15100/25252], Loss: 3493.6544\n",
      "Epoch [1/3], Step [15200/25252], Loss: 3144.3574\n",
      "Epoch [1/3], Step [15300/25252], Loss: 3881.1109\n",
      "Epoch [1/3], Step [15400/25252], Loss: 3376.0605\n",
      "Epoch [1/3], Step [15500/25252], Loss: 3610.8724\n",
      "Epoch [1/3], Step [15600/25252], Loss: 3658.1860\n",
      "Epoch [1/3], Step [15700/25252], Loss: 4363.2996\n",
      "Epoch [1/3], Step [15800/25252], Loss: 4566.3180\n",
      "Epoch [1/3], Step [15900/25252], Loss: 3014.8324\n",
      "Epoch [1/3], Step [16000/25252], Loss: 3482.6731\n",
      "Epoch [1/3], Step [16100/25252], Loss: 4448.4286\n",
      "Epoch [1/3], Step [16200/25252], Loss: 3923.7605\n",
      "Epoch [1/3], Step [16300/25252], Loss: 2988.2462\n",
      "Epoch [1/3], Step [16400/25252], Loss: 4310.3987\n",
      "Epoch [1/3], Step [16500/25252], Loss: 2876.6606\n",
      "Epoch [1/3], Step [16600/25252], Loss: 3635.4627\n",
      "Epoch [1/3], Step [16700/25252], Loss: 3987.3291\n",
      "Epoch [1/3], Step [16800/25252], Loss: 3248.0375\n",
      "Epoch [1/3], Step [16900/25252], Loss: 4323.2287\n",
      "Epoch [1/3], Step [17000/25252], Loss: 2985.6573\n",
      "Epoch [1/3], Step [17100/25252], Loss: 3486.9176\n",
      "Epoch [1/3], Step [17200/25252], Loss: 4111.7740\n",
      "Epoch [1/3], Step [17300/25252], Loss: 3923.2143\n",
      "Epoch [1/3], Step [17400/25252], Loss: 3438.3327\n",
      "Epoch [1/3], Step [17500/25252], Loss: 3809.1693\n",
      "Epoch [1/3], Step [17600/25252], Loss: 3630.7032\n",
      "Epoch [1/3], Step [17700/25252], Loss: 3622.8817\n",
      "Epoch [1/3], Step [17800/25252], Loss: 4312.1663\n",
      "Epoch [1/3], Step [17900/25252], Loss: 4282.9023\n",
      "Epoch [1/3], Step [18000/25252], Loss: 2952.8537\n",
      "Epoch [1/3], Step [18100/25252], Loss: 2971.5377\n",
      "Epoch [1/3], Step [18200/25252], Loss: 3978.4468\n",
      "Epoch [1/3], Step [18300/25252], Loss: 3637.1112\n",
      "Epoch [1/3], Step [18400/25252], Loss: 3150.7146\n",
      "Epoch [1/3], Step [18500/25252], Loss: 4097.2544\n",
      "Epoch [1/3], Step [18600/25252], Loss: 3821.8081\n",
      "Epoch [1/3], Step [18700/25252], Loss: 3484.3719\n",
      "Epoch [1/3], Step [18800/25252], Loss: 3293.9090\n",
      "Epoch [1/3], Step [18900/25252], Loss: 3300.4132\n",
      "Epoch [1/3], Step [19000/25252], Loss: 3834.1050\n",
      "Epoch [1/3], Step [19100/25252], Loss: 3447.2435\n",
      "Epoch [1/3], Step [19200/25252], Loss: 4879.5636\n",
      "Epoch [1/3], Step [19300/25252], Loss: 3573.9076\n",
      "Epoch [1/3], Step [19400/25252], Loss: 3918.3262\n",
      "Epoch [1/3], Step [19500/25252], Loss: 4011.9069\n",
      "Epoch [1/3], Step [19600/25252], Loss: 4260.2748\n",
      "Epoch [1/3], Step [19700/25252], Loss: 4651.9957\n",
      "Epoch [1/3], Step [19800/25252], Loss: 3521.8737\n",
      "Epoch [1/3], Step [19900/25252], Loss: 4937.6145\n",
      "Epoch [1/3], Step [20000/25252], Loss: 3531.3939\n",
      "Epoch [1/3], Step [20100/25252], Loss: 3262.2410\n",
      "Epoch [1/3], Step [20200/25252], Loss: 4435.3052\n",
      "Epoch [1/3], Step [20300/25252], Loss: 3200.8674\n",
      "Epoch [1/3], Step [20400/25252], Loss: 3643.6296\n",
      "Epoch [1/3], Step [20500/25252], Loss: 4920.8763\n",
      "Epoch [1/3], Step [20600/25252], Loss: 2836.9986\n",
      "Epoch [1/3], Step [20700/25252], Loss: 3830.6332\n",
      "Epoch [1/3], Step [20800/25252], Loss: 3980.3144\n",
      "Epoch [1/3], Step [20900/25252], Loss: 4275.8758\n",
      "Epoch [1/3], Step [21000/25252], Loss: 3472.8268\n",
      "Epoch [1/3], Step [21100/25252], Loss: 3833.2290\n",
      "Epoch [1/3], Step [21200/25252], Loss: 2935.6756\n",
      "Epoch [1/3], Step [21300/25252], Loss: 3422.3140\n",
      "Epoch [1/3], Step [21400/25252], Loss: 3645.1910\n",
      "Epoch [1/3], Step [21500/25252], Loss: 4650.7056\n",
      "Epoch [1/3], Step [21600/25252], Loss: 3993.8870\n",
      "Epoch [1/3], Step [21700/25252], Loss: 3379.2013\n",
      "Epoch [1/3], Step [21800/25252], Loss: 3954.4575\n",
      "Epoch [1/3], Step [21900/25252], Loss: 3560.9274\n",
      "Epoch [1/3], Step [22000/25252], Loss: 3629.7577\n",
      "Epoch [1/3], Step [22100/25252], Loss: 4300.9706\n",
      "Epoch [1/3], Step [22200/25252], Loss: 3916.1848\n",
      "Epoch [1/3], Step [22300/25252], Loss: 3939.6366\n",
      "Epoch [1/3], Step [22400/25252], Loss: 3007.5107\n",
      "Epoch [1/3], Step [22500/25252], Loss: 4877.7292\n",
      "Epoch [1/3], Step [22600/25252], Loss: 3400.8994\n",
      "Epoch [1/3], Step [22700/25252], Loss: 3380.3646\n",
      "Epoch [1/3], Step [22800/25252], Loss: 3481.7641\n",
      "Epoch [1/3], Step [22900/25252], Loss: 3491.2617\n",
      "Epoch [1/3], Step [23000/25252], Loss: 4034.9596\n",
      "Epoch [1/3], Step [23100/25252], Loss: 3767.9604\n",
      "Epoch [1/3], Step [23200/25252], Loss: 4682.2285\n",
      "Epoch [1/3], Step [23300/25252], Loss: 3410.4879\n",
      "Epoch [1/3], Step [23400/25252], Loss: 4121.9606\n",
      "Epoch [1/3], Step [23500/25252], Loss: 3080.7892\n",
      "Epoch [1/3], Step [23600/25252], Loss: 4623.8736\n",
      "Epoch [1/3], Step [23700/25252], Loss: 6239.1916\n",
      "Epoch [1/3], Step [23800/25252], Loss: 3717.7405\n",
      "Epoch [1/3], Step [23900/25252], Loss: 4450.5265\n",
      "Epoch [1/3], Step [24000/25252], Loss: 4547.7028\n",
      "Epoch [1/3], Step [24100/25252], Loss: 2889.1610\n",
      "Epoch [1/3], Step [24200/25252], Loss: 3174.1870\n",
      "Epoch [1/3], Step [24300/25252], Loss: 4138.8406\n",
      "Epoch [1/3], Step [24400/25252], Loss: 3954.3733\n",
      "Epoch [1/3], Step [24500/25252], Loss: 3190.8855\n",
      "Epoch [1/3], Step [24600/25252], Loss: 3894.7060\n",
      "Epoch [1/3], Step [24700/25252], Loss: 3277.1952\n",
      "Epoch [1/3], Step [24800/25252], Loss: 4533.8689\n",
      "Epoch [1/3], Step [24900/25252], Loss: 5154.2623\n",
      "Epoch [1/3], Step [25000/25252], Loss: 4375.5939\n",
      "Epoch [1/3], Step [25100/25252], Loss: 3271.6432\n",
      "Epoch [1/3], Step [25200/25252], Loss: 4267.2217\n",
      "Epoch [2/3], Step [100/25252], Loss: 3706.8072\n",
      "Epoch [2/3], Step [200/25252], Loss: 3669.0173\n",
      "Epoch [2/3], Step [300/25252], Loss: 2879.4714\n",
      "Epoch [2/3], Step [400/25252], Loss: 3673.4803\n",
      "Epoch [2/3], Step [500/25252], Loss: 3932.8854\n",
      "Epoch [2/3], Step [600/25252], Loss: 4002.6884\n",
      "Epoch [2/3], Step [700/25252], Loss: 3689.3191\n",
      "Epoch [2/3], Step [800/25252], Loss: 3259.6703\n",
      "Epoch [2/3], Step [900/25252], Loss: 3216.1432\n",
      "Epoch [2/3], Step [1000/25252], Loss: 3246.9696\n",
      "Epoch [2/3], Step [1100/25252], Loss: 3843.3968\n",
      "Epoch [2/3], Step [1200/25252], Loss: 3883.8289\n",
      "Epoch [2/3], Step [1300/25252], Loss: 3870.9652\n",
      "Epoch [2/3], Step [1400/25252], Loss: 3367.8108\n",
      "Epoch [2/3], Step [1500/25252], Loss: 4435.4000\n",
      "Epoch [2/3], Step [1600/25252], Loss: 6432.5252\n",
      "Epoch [2/3], Step [1700/25252], Loss: 3828.8911\n",
      "Epoch [2/3], Step [1800/25252], Loss: 4516.8557\n",
      "Epoch [2/3], Step [1900/25252], Loss: 3538.7790\n",
      "Epoch [2/3], Step [2000/25252], Loss: 3172.7661\n",
      "Epoch [2/3], Step [2100/25252], Loss: 2959.6987\n",
      "Epoch [2/3], Step [2200/25252], Loss: 2972.0324\n",
      "Epoch [2/3], Step [2300/25252], Loss: 3865.8168\n",
      "Epoch [2/3], Step [2400/25252], Loss: 4316.4345\n",
      "Epoch [2/3], Step [2500/25252], Loss: 4218.4156\n",
      "Epoch [2/3], Step [2600/25252], Loss: 3461.0131\n",
      "Epoch [2/3], Step [2700/25252], Loss: 4375.5611\n",
      "Epoch [2/3], Step [2800/25252], Loss: 3934.4911\n",
      "Epoch [2/3], Step [2900/25252], Loss: 3133.3158\n",
      "Epoch [2/3], Step [3000/25252], Loss: 3080.7952\n",
      "Epoch [2/3], Step [3100/25252], Loss: 3053.5662\n",
      "Epoch [2/3], Step [3200/25252], Loss: 4281.3974\n",
      "Epoch [2/3], Step [3300/25252], Loss: 2871.9638\n",
      "Epoch [2/3], Step [3400/25252], Loss: 3727.9109\n",
      "Epoch [2/3], Step [3500/25252], Loss: 3179.0504\n",
      "Epoch [2/3], Step [3600/25252], Loss: 3008.0731\n",
      "Epoch [2/3], Step [3700/25252], Loss: 4195.1248\n",
      "Epoch [2/3], Step [3800/25252], Loss: 3260.6434\n",
      "Epoch [2/3], Step [3900/25252], Loss: 4319.4115\n",
      "Epoch [2/3], Step [4000/25252], Loss: 4001.0419\n",
      "Epoch [2/3], Step [4100/25252], Loss: 3085.8956\n",
      "Epoch [2/3], Step [4200/25252], Loss: 3247.8419\n",
      "Epoch [2/3], Step [4300/25252], Loss: 2567.5608\n",
      "Epoch [2/3], Step [4400/25252], Loss: 3473.2800\n",
      "Epoch [2/3], Step [4500/25252], Loss: 3876.9294\n",
      "Epoch [2/3], Step [4600/25252], Loss: 3843.8807\n",
      "Epoch [2/3], Step [4700/25252], Loss: 5211.7919\n",
      "Epoch [2/3], Step [4800/25252], Loss: 2933.7193\n",
      "Epoch [2/3], Step [4900/25252], Loss: 3442.6290\n",
      "Epoch [2/3], Step [5000/25252], Loss: 4170.6112\n",
      "Epoch [2/3], Step [5100/25252], Loss: 3163.2142\n",
      "Epoch [2/3], Step [5200/25252], Loss: 3318.1215\n",
      "Epoch [2/3], Step [5300/25252], Loss: 4198.4970\n",
      "Epoch [2/3], Step [5400/25252], Loss: 4203.3686\n",
      "Epoch [2/3], Step [5500/25252], Loss: 3283.8182\n",
      "Epoch [2/3], Step [5600/25252], Loss: 3633.7258\n",
      "Epoch [2/3], Step [5700/25252], Loss: 4207.1606\n",
      "Epoch [2/3], Step [5800/25252], Loss: 3847.3949\n",
      "Epoch [2/3], Step [5900/25252], Loss: 4736.1914\n",
      "Epoch [2/3], Step [6000/25252], Loss: 3887.7396\n",
      "Epoch [2/3], Step [6100/25252], Loss: 4263.8718\n",
      "Epoch [2/3], Step [6200/25252], Loss: 4154.8209\n",
      "Epoch [2/3], Step [6300/25252], Loss: 3189.0848\n",
      "Epoch [2/3], Step [6400/25252], Loss: 4802.9018\n",
      "Epoch [2/3], Step [6500/25252], Loss: 3091.7850\n",
      "Epoch [2/3], Step [6600/25252], Loss: 3705.7919\n",
      "Epoch [2/3], Step [6700/25252], Loss: 5410.1875\n",
      "Epoch [2/3], Step [6800/25252], Loss: 4669.0955\n",
      "Epoch [2/3], Step [6900/25252], Loss: 4070.6362\n",
      "Epoch [2/3], Step [7000/25252], Loss: 3222.1817\n",
      "Epoch [2/3], Step [7100/25252], Loss: 3266.4509\n",
      "Epoch [2/3], Step [7200/25252], Loss: 3497.3460\n",
      "Epoch [2/3], Step [7300/25252], Loss: 3563.6399\n",
      "Epoch [2/3], Step [7400/25252], Loss: 4252.6865\n",
      "Epoch [2/3], Step [7500/25252], Loss: 4201.4508\n",
      "Epoch [2/3], Step [7600/25252], Loss: 3814.0701\n",
      "Epoch [2/3], Step [7700/25252], Loss: 3799.6474\n",
      "Epoch [2/3], Step [7800/25252], Loss: 3171.0101\n",
      "Epoch [2/3], Step [7900/25252], Loss: 2935.4261\n",
      "Epoch [2/3], Step [8000/25252], Loss: 3798.4298\n",
      "Epoch [2/3], Step [8100/25252], Loss: 3977.2375\n",
      "Epoch [2/3], Step [8200/25252], Loss: 3463.8689\n",
      "Epoch [2/3], Step [8300/25252], Loss: 3395.6621\n",
      "Epoch [2/3], Step [8400/25252], Loss: 3433.2517\n",
      "Epoch [2/3], Step [8500/25252], Loss: 2962.6206\n",
      "Epoch [2/3], Step [8600/25252], Loss: 4366.2626\n",
      "Epoch [2/3], Step [8700/25252], Loss: 4027.2532\n",
      "Epoch [2/3], Step [8800/25252], Loss: 3298.8306\n",
      "Epoch [2/3], Step [8900/25252], Loss: 5031.4715\n",
      "Epoch [2/3], Step [9000/25252], Loss: 4177.2291\n",
      "Epoch [2/3], Step [9100/25252], Loss: 3389.1706\n",
      "Epoch [2/3], Step [9200/25252], Loss: 3295.0606\n",
      "Epoch [2/3], Step [9300/25252], Loss: 4391.9554\n",
      "Epoch [2/3], Step [9400/25252], Loss: 2753.1261\n",
      "Epoch [2/3], Step [9500/25252], Loss: 3765.8551\n",
      "Epoch [2/3], Step [9600/25252], Loss: 4103.9326\n",
      "Epoch [2/3], Step [9700/25252], Loss: 3491.6489\n",
      "Epoch [2/3], Step [9800/25252], Loss: 2941.1875\n",
      "Epoch [2/3], Step [9900/25252], Loss: 3564.6343\n",
      "Epoch [2/3], Step [10000/25252], Loss: 3007.4399\n",
      "Epoch [2/3], Step [10100/25252], Loss: 3990.5991\n",
      "Epoch [2/3], Step [10200/25252], Loss: 3762.0360\n",
      "Epoch [2/3], Step [10300/25252], Loss: 3767.2740\n",
      "Epoch [2/3], Step [10400/25252], Loss: 3823.3814\n",
      "Epoch [2/3], Step [10500/25252], Loss: 3785.2126\n",
      "Epoch [2/3], Step [10600/25252], Loss: 3353.8812\n",
      "Epoch [2/3], Step [10700/25252], Loss: 3370.0116\n",
      "Epoch [2/3], Step [10800/25252], Loss: 3255.9903\n",
      "Epoch [2/3], Step [10900/25252], Loss: 3986.6915\n",
      "Epoch [2/3], Step [11000/25252], Loss: 3478.3062\n",
      "Epoch [2/3], Step [11100/25252], Loss: 5256.6209\n",
      "Epoch [2/3], Step [11200/25252], Loss: 4235.4199\n",
      "Epoch [2/3], Step [11300/25252], Loss: 3245.9831\n",
      "Epoch [2/3], Step [11400/25252], Loss: 3694.7455\n",
      "Epoch [2/3], Step [11500/25252], Loss: 2778.0453\n",
      "Epoch [2/3], Step [11600/25252], Loss: 5666.6261\n",
      "Epoch [2/3], Step [11700/25252], Loss: 4384.8492\n",
      "Epoch [2/3], Step [11800/25252], Loss: 3206.8682\n",
      "Epoch [2/3], Step [11900/25252], Loss: 3438.1546\n",
      "Epoch [2/3], Step [12000/25252], Loss: 3610.2770\n",
      "Epoch [2/3], Step [12100/25252], Loss: 3265.5699\n",
      "Epoch [2/3], Step [12200/25252], Loss: 2964.3730\n",
      "Epoch [2/3], Step [12300/25252], Loss: 4322.9529\n",
      "Epoch [2/3], Step [12400/25252], Loss: 2882.0936\n",
      "Epoch [2/3], Step [12500/25252], Loss: 4130.8898\n",
      "Epoch [2/3], Step [12600/25252], Loss: 5427.3458\n",
      "Epoch [2/3], Step [12700/25252], Loss: 2993.0210\n",
      "Epoch [2/3], Step [12800/25252], Loss: 3487.9047\n",
      "Epoch [2/3], Step [12900/25252], Loss: 2649.1686\n",
      "Epoch [2/3], Step [13000/25252], Loss: 3630.5974\n",
      "Epoch [2/3], Step [13100/25252], Loss: 4437.5235\n",
      "Epoch [2/3], Step [13200/25252], Loss: 4235.1820\n",
      "Epoch [2/3], Step [13300/25252], Loss: 4187.5196\n",
      "Epoch [2/3], Step [13400/25252], Loss: 3686.5188\n",
      "Epoch [2/3], Step [13500/25252], Loss: 2745.1161\n",
      "Epoch [2/3], Step [13600/25252], Loss: 3511.2423\n",
      "Epoch [2/3], Step [13700/25252], Loss: 3723.7762\n",
      "Epoch [2/3], Step [13800/25252], Loss: 3576.7537\n",
      "Epoch [2/3], Step [13900/25252], Loss: 4111.2246\n",
      "Epoch [2/3], Step [14000/25252], Loss: 2966.3781\n",
      "Epoch [2/3], Step [14100/25252], Loss: 4826.2573\n",
      "Epoch [2/3], Step [14200/25252], Loss: 3411.2398\n",
      "Epoch [2/3], Step [14300/25252], Loss: 4483.0857\n",
      "Epoch [2/3], Step [14400/25252], Loss: 3171.9323\n",
      "Epoch [2/3], Step [14500/25252], Loss: 4908.0932\n",
      "Epoch [2/3], Step [14600/25252], Loss: 3201.3472\n",
      "Epoch [2/3], Step [14700/25252], Loss: 3960.9808\n",
      "Epoch [2/3], Step [14800/25252], Loss: 3969.8611\n",
      "Epoch [2/3], Step [14900/25252], Loss: 3126.1483\n",
      "Epoch [2/3], Step [15000/25252], Loss: 3662.8523\n",
      "Epoch [2/3], Step [15100/25252], Loss: 3362.3092\n",
      "Epoch [2/3], Step [15200/25252], Loss: 2969.2304\n",
      "Epoch [2/3], Step [15300/25252], Loss: 3945.9031\n",
      "Epoch [2/3], Step [15400/25252], Loss: 3906.0460\n",
      "Epoch [2/3], Step [15500/25252], Loss: 3458.6303\n",
      "Epoch [2/3], Step [15600/25252], Loss: 3455.4744\n",
      "Epoch [2/3], Step [15700/25252], Loss: 3766.9150\n",
      "Epoch [2/3], Step [15800/25252], Loss: 3387.3183\n",
      "Epoch [2/3], Step [15900/25252], Loss: 3786.3266\n",
      "Epoch [2/3], Step [16000/25252], Loss: 3811.8532\n",
      "Epoch [2/3], Step [16100/25252], Loss: 4570.6675\n",
      "Epoch [2/3], Step [16200/25252], Loss: 3393.3059\n",
      "Epoch [2/3], Step [16300/25252], Loss: 3428.8017\n",
      "Epoch [2/3], Step [16400/25252], Loss: 3869.3192\n",
      "Epoch [2/3], Step [16500/25252], Loss: 4019.7214\n",
      "Epoch [2/3], Step [16600/25252], Loss: 3288.6700\n",
      "Epoch [2/3], Step [16700/25252], Loss: 2674.4538\n",
      "Epoch [2/3], Step [16800/25252], Loss: 3967.7047\n",
      "Epoch [2/3], Step [16900/25252], Loss: 3744.5663\n",
      "Epoch [2/3], Step [17000/25252], Loss: 3429.7729\n",
      "Epoch [2/3], Step [17100/25252], Loss: 5118.2993\n",
      "Epoch [2/3], Step [17200/25252], Loss: 3729.3806\n",
      "Epoch [2/3], Step [17300/25252], Loss: 3627.8482\n",
      "Epoch [2/3], Step [17400/25252], Loss: 3338.0744\n",
      "Epoch [2/3], Step [17500/25252], Loss: 3623.1522\n",
      "Epoch [2/3], Step [17600/25252], Loss: 3547.0361\n",
      "Epoch [2/3], Step [17700/25252], Loss: 4743.2838\n",
      "Epoch [2/3], Step [17800/25252], Loss: 3803.5261\n",
      "Epoch [2/3], Step [17900/25252], Loss: 3769.1325\n",
      "Epoch [2/3], Step [18000/25252], Loss: 3700.0617\n",
      "Epoch [2/3], Step [18100/25252], Loss: 4190.7852\n",
      "Epoch [2/3], Step [18200/25252], Loss: 4205.7107\n",
      "Epoch [2/3], Step [18300/25252], Loss: 3162.1004\n",
      "Epoch [2/3], Step [18400/25252], Loss: 2543.5710\n",
      "Epoch [2/3], Step [18500/25252], Loss: 3533.4265\n",
      "Epoch [2/3], Step [18600/25252], Loss: 3634.7299\n",
      "Epoch [2/3], Step [18700/25252], Loss: 3585.1453\n",
      "Epoch [2/3], Step [18800/25252], Loss: 4232.6210\n",
      "Epoch [2/3], Step [18900/25252], Loss: 3148.6864\n",
      "Epoch [2/3], Step [19000/25252], Loss: 3965.2427\n",
      "Epoch [2/3], Step [19100/25252], Loss: 4122.6907\n",
      "Epoch [2/3], Step [19200/25252], Loss: 3705.5824\n",
      "Epoch [2/3], Step [19300/25252], Loss: 3379.8185\n",
      "Epoch [2/3], Step [19400/25252], Loss: 3231.2388\n",
      "Epoch [2/3], Step [19500/25252], Loss: 3681.3806\n",
      "Epoch [2/3], Step [19600/25252], Loss: 3300.0224\n",
      "Epoch [2/3], Step [19700/25252], Loss: 4027.8843\n",
      "Epoch [2/3], Step [19800/25252], Loss: 3566.2082\n",
      "Epoch [2/3], Step [19900/25252], Loss: 3224.8918\n",
      "Epoch [2/3], Step [20000/25252], Loss: 2998.7038\n",
      "Epoch [2/3], Step [20100/25252], Loss: 3460.9302\n",
      "Epoch [2/3], Step [20200/25252], Loss: 5781.4225\n",
      "Epoch [2/3], Step [20300/25252], Loss: 3409.7368\n",
      "Epoch [2/3], Step [20400/25252], Loss: 3352.0236\n",
      "Epoch [2/3], Step [20500/25252], Loss: 3498.9405\n",
      "Epoch [2/3], Step [20600/25252], Loss: 3316.7271\n",
      "Epoch [2/3], Step [20700/25252], Loss: 2929.4302\n",
      "Epoch [2/3], Step [20800/25252], Loss: 3176.8748\n",
      "Epoch [2/3], Step [20900/25252], Loss: 3581.4001\n",
      "Epoch [2/3], Step [21000/25252], Loss: 3745.1586\n",
      "Epoch [2/3], Step [21100/25252], Loss: 3816.1252\n",
      "Epoch [2/3], Step [21200/25252], Loss: 3548.0510\n",
      "Epoch [2/3], Step [21300/25252], Loss: 4550.8217\n",
      "Epoch [2/3], Step [21400/25252], Loss: 4199.3391\n",
      "Epoch [2/3], Step [21500/25252], Loss: 3812.8507\n",
      "Epoch [2/3], Step [21600/25252], Loss: 3474.6520\n",
      "Epoch [2/3], Step [21700/25252], Loss: 3974.5942\n",
      "Epoch [2/3], Step [21800/25252], Loss: 3143.8605\n",
      "Epoch [2/3], Step [21900/25252], Loss: 4440.9986\n",
      "Epoch [2/3], Step [22000/25252], Loss: 4361.3094\n",
      "Epoch [2/3], Step [22100/25252], Loss: 3215.0555\n",
      "Epoch [2/3], Step [22200/25252], Loss: 3949.6275\n",
      "Epoch [2/3], Step [22300/25252], Loss: 5003.8061\n",
      "Epoch [2/3], Step [22400/25252], Loss: 3997.6036\n",
      "Epoch [2/3], Step [22500/25252], Loss: 4013.0100\n",
      "Epoch [2/3], Step [22600/25252], Loss: 2880.8712\n",
      "Epoch [2/3], Step [22700/25252], Loss: 4019.8179\n",
      "Epoch [2/3], Step [22800/25252], Loss: 4062.2730\n",
      "Epoch [2/3], Step [22900/25252], Loss: 3185.2033\n",
      "Epoch [2/3], Step [23000/25252], Loss: 3552.4454\n",
      "Epoch [2/3], Step [23100/25252], Loss: 3431.3586\n",
      "Epoch [2/3], Step [23200/25252], Loss: 4208.8529\n",
      "Epoch [2/3], Step [23300/25252], Loss: 3492.0342\n",
      "Epoch [2/3], Step [23400/25252], Loss: 3714.3127\n",
      "Epoch [2/3], Step [23500/25252], Loss: 3359.3941\n",
      "Epoch [2/3], Step [23600/25252], Loss: 3490.6597\n",
      "Epoch [2/3], Step [23700/25252], Loss: 3363.3726\n",
      "Epoch [2/3], Step [23800/25252], Loss: 3105.6873\n",
      "Epoch [2/3], Step [23900/25252], Loss: 3985.0083\n",
      "Epoch [2/3], Step [24000/25252], Loss: 2744.1898\n",
      "Epoch [2/3], Step [24100/25252], Loss: 5006.9252\n",
      "Epoch [2/3], Step [24200/25252], Loss: 3114.7867\n",
      "Epoch [2/3], Step [24300/25252], Loss: 4812.5191\n",
      "Epoch [2/3], Step [24400/25252], Loss: 2936.6234\n",
      "Epoch [2/3], Step [24500/25252], Loss: 3567.7804\n",
      "Epoch [2/3], Step [24600/25252], Loss: 2979.3859\n",
      "Epoch [2/3], Step [24700/25252], Loss: 3571.1649\n",
      "Epoch [2/3], Step [24800/25252], Loss: 3750.1013\n",
      "Epoch [2/3], Step [24900/25252], Loss: 4050.8757\n",
      "Epoch [2/3], Step [25000/25252], Loss: 3882.9752\n",
      "Epoch [2/3], Step [25100/25252], Loss: 3306.1119\n",
      "Epoch [2/3], Step [25200/25252], Loss: 4330.9074\n",
      "Epoch [3/3], Step [100/25252], Loss: 3084.1272\n",
      "Epoch [3/3], Step [200/25252], Loss: 3564.8339\n",
      "Epoch [3/3], Step [300/25252], Loss: 3355.2740\n",
      "Epoch [3/3], Step [400/25252], Loss: 3194.5201\n",
      "Epoch [3/3], Step [500/25252], Loss: 4012.0543\n",
      "Epoch [3/3], Step [600/25252], Loss: 3163.9127\n",
      "Epoch [3/3], Step [700/25252], Loss: 3296.2469\n",
      "Epoch [3/3], Step [800/25252], Loss: 3365.1344\n",
      "Epoch [3/3], Step [900/25252], Loss: 3624.3556\n",
      "Epoch [3/3], Step [1000/25252], Loss: 3181.6747\n",
      "Epoch [3/3], Step [1100/25252], Loss: 3660.3544\n",
      "Epoch [3/3], Step [1200/25252], Loss: 4697.5313\n",
      "Epoch [3/3], Step [1300/25252], Loss: 3334.2153\n",
      "Epoch [3/3], Step [1400/25252], Loss: 3801.5303\n",
      "Epoch [3/3], Step [1500/25252], Loss: 3307.7910\n",
      "Epoch [3/3], Step [1600/25252], Loss: 2790.1220\n",
      "Epoch [3/3], Step [1700/25252], Loss: 2591.7671\n",
      "Epoch [3/3], Step [1800/25252], Loss: 3362.3863\n",
      "Epoch [3/3], Step [1900/25252], Loss: 2685.5218\n",
      "Epoch [3/3], Step [2000/25252], Loss: 4927.4689\n",
      "Epoch [3/3], Step [2100/25252], Loss: 4256.5273\n",
      "Epoch [3/3], Step [2200/25252], Loss: 3279.5353\n",
      "Epoch [3/3], Step [2300/25252], Loss: 4056.6585\n",
      "Epoch [3/3], Step [2400/25252], Loss: 4677.0434\n",
      "Epoch [3/3], Step [2500/25252], Loss: 3868.6852\n",
      "Epoch [3/3], Step [2600/25252], Loss: 4159.4453\n",
      "Epoch [3/3], Step [2700/25252], Loss: 3107.4473\n",
      "Epoch [3/3], Step [2800/25252], Loss: 3544.7103\n",
      "Epoch [3/3], Step [2900/25252], Loss: 3127.8586\n",
      "Epoch [3/3], Step [3000/25252], Loss: 3462.2274\n",
      "Epoch [3/3], Step [3100/25252], Loss: 2932.7268\n",
      "Epoch [3/3], Step [3200/25252], Loss: 3685.5704\n",
      "Epoch [3/3], Step [3300/25252], Loss: 4332.7025\n",
      "Epoch [3/3], Step [3400/25252], Loss: 2859.0660\n",
      "Epoch [3/3], Step [3500/25252], Loss: 3009.0958\n",
      "Epoch [3/3], Step [3600/25252], Loss: 3345.7936\n",
      "Epoch [3/3], Step [3700/25252], Loss: 3259.1288\n",
      "Epoch [3/3], Step [3800/25252], Loss: 4614.6936\n",
      "Epoch [3/3], Step [3900/25252], Loss: 4950.7227\n",
      "Epoch [3/3], Step [4000/25252], Loss: 3606.1906\n",
      "Epoch [3/3], Step [4100/25252], Loss: 2954.0836\n",
      "Epoch [3/3], Step [4200/25252], Loss: 4427.4487\n",
      "Epoch [3/3], Step [4300/25252], Loss: 4160.5313\n",
      "Epoch [3/3], Step [4400/25252], Loss: 3970.7353\n",
      "Epoch [3/3], Step [4500/25252], Loss: 2756.7694\n",
      "Epoch [3/3], Step [4600/25252], Loss: 3342.9405\n",
      "Epoch [3/3], Step [4700/25252], Loss: 3939.4852\n",
      "Epoch [3/3], Step [4800/25252], Loss: 3283.1896\n",
      "Epoch [3/3], Step [4900/25252], Loss: 4520.2678\n",
      "Epoch [3/3], Step [5000/25252], Loss: 3372.2321\n",
      "Epoch [3/3], Step [5100/25252], Loss: 3449.3090\n",
      "Epoch [3/3], Step [5200/25252], Loss: 3959.3438\n",
      "Epoch [3/3], Step [5300/25252], Loss: 4533.0642\n",
      "Epoch [3/3], Step [5400/25252], Loss: 4641.3862\n",
      "Epoch [3/3], Step [5500/25252], Loss: 5025.9886\n",
      "Epoch [3/3], Step [5600/25252], Loss: 3450.5124\n",
      "Epoch [3/3], Step [5700/25252], Loss: 3307.9292\n",
      "Epoch [3/3], Step [5800/25252], Loss: 3879.1008\n",
      "Epoch [3/3], Step [5900/25252], Loss: 3670.7465\n",
      "Epoch [3/3], Step [6000/25252], Loss: 3310.2887\n",
      "Epoch [3/3], Step [6100/25252], Loss: 3733.4435\n",
      "Epoch [3/3], Step [6200/25252], Loss: 4098.9491\n",
      "Epoch [3/3], Step [6300/25252], Loss: 3256.9365\n",
      "Epoch [3/3], Step [6400/25252], Loss: 3287.4526\n",
      "Epoch [3/3], Step [6500/25252], Loss: 3533.4523\n",
      "Epoch [3/3], Step [6600/25252], Loss: 5342.5505\n",
      "Epoch [3/3], Step [6700/25252], Loss: 3490.7435\n",
      "Epoch [3/3], Step [6800/25252], Loss: 2983.8535\n",
      "Epoch [3/3], Step [6900/25252], Loss: 3499.2162\n",
      "Epoch [3/3], Step [7000/25252], Loss: 4452.3238\n",
      "Epoch [3/3], Step [7100/25252], Loss: 4603.8681\n",
      "Epoch [3/3], Step [7200/25252], Loss: 4275.8409\n",
      "Epoch [3/3], Step [7300/25252], Loss: 3901.2400\n",
      "Epoch [3/3], Step [7400/25252], Loss: 3417.8758\n",
      "Epoch [3/3], Step [7500/25252], Loss: 3215.1019\n",
      "Epoch [3/3], Step [7600/25252], Loss: 3712.4627\n",
      "Epoch [3/3], Step [7700/25252], Loss: 3727.3652\n",
      "Epoch [3/3], Step [7800/25252], Loss: 3479.1415\n",
      "Epoch [3/3], Step [7900/25252], Loss: 4101.4973\n",
      "Epoch [3/3], Step [8000/25252], Loss: 3573.8534\n",
      "Epoch [3/3], Step [8100/25252], Loss: 3352.8302\n",
      "Epoch [3/3], Step [8200/25252], Loss: 4081.3885\n",
      "Epoch [3/3], Step [8300/25252], Loss: 3814.4105\n",
      "Epoch [3/3], Step [8400/25252], Loss: 6576.6794\n",
      "Epoch [3/3], Step [8500/25252], Loss: 4034.6722\n",
      "Epoch [3/3], Step [8600/25252], Loss: 3660.3701\n",
      "Epoch [3/3], Step [8700/25252], Loss: 2928.2352\n",
      "Epoch [3/3], Step [8800/25252], Loss: 6013.4734\n",
      "Epoch [3/3], Step [8900/25252], Loss: 3819.8674\n",
      "Epoch [3/3], Step [9000/25252], Loss: 3294.7711\n",
      "Epoch [3/3], Step [9100/25252], Loss: 3657.8966\n",
      "Epoch [3/3], Step [9200/25252], Loss: 3183.9613\n",
      "Epoch [3/3], Step [9300/25252], Loss: 3719.3117\n",
      "Epoch [3/3], Step [9400/25252], Loss: 3779.3252\n",
      "Epoch [3/3], Step [9500/25252], Loss: 3328.5175\n",
      "Epoch [3/3], Step [9600/25252], Loss: 3863.4651\n",
      "Epoch [3/3], Step [9700/25252], Loss: 4562.4909\n",
      "Epoch [3/3], Step [9800/25252], Loss: 3351.5883\n",
      "Epoch [3/3], Step [9900/25252], Loss: 3591.3481\n",
      "Epoch [3/3], Step [10000/25252], Loss: 2903.9684\n",
      "Epoch [3/3], Step [10100/25252], Loss: 3835.5085\n",
      "Epoch [3/3], Step [10200/25252], Loss: 3190.9446\n",
      "Epoch [3/3], Step [10300/25252], Loss: 3437.0778\n",
      "Epoch [3/3], Step [10400/25252], Loss: 4063.2298\n",
      "Epoch [3/3], Step [10500/25252], Loss: 3779.1137\n",
      "Epoch [3/3], Step [10600/25252], Loss: 2839.9654\n",
      "Epoch [3/3], Step [10700/25252], Loss: 3848.1118\n",
      "Epoch [3/3], Step [10800/25252], Loss: 3135.3921\n",
      "Epoch [3/3], Step [10900/25252], Loss: 3320.2238\n",
      "Epoch [3/3], Step [11000/25252], Loss: 3536.0718\n",
      "Epoch [3/3], Step [11100/25252], Loss: 4898.4775\n",
      "Epoch [3/3], Step [11200/25252], Loss: 3620.1160\n",
      "Epoch [3/3], Step [11300/25252], Loss: 3110.0731\n",
      "Epoch [3/3], Step [11400/25252], Loss: 4049.5798\n",
      "Epoch [3/3], Step [11500/25252], Loss: 4400.5383\n",
      "Epoch [3/3], Step [11600/25252], Loss: 3387.2494\n",
      "Epoch [3/3], Step [11700/25252], Loss: 4112.5973\n",
      "Epoch [3/3], Step [11800/25252], Loss: 3405.2211\n",
      "Epoch [3/3], Step [11900/25252], Loss: 4194.2207\n",
      "Epoch [3/3], Step [12000/25252], Loss: 3792.2313\n",
      "Epoch [3/3], Step [12100/25252], Loss: 4282.1442\n",
      "Epoch [3/3], Step [12200/25252], Loss: 3068.9032\n",
      "Epoch [3/3], Step [12300/25252], Loss: 3418.3432\n",
      "Epoch [3/3], Step [12400/25252], Loss: 3524.8980\n",
      "Epoch [3/3], Step [12500/25252], Loss: 3826.1241\n",
      "Epoch [3/3], Step [12600/25252], Loss: 3391.3748\n",
      "Epoch [3/3], Step [12700/25252], Loss: 3003.7803\n",
      "Epoch [3/3], Step [12800/25252], Loss: 3916.8915\n",
      "Epoch [3/3], Step [12900/25252], Loss: 4149.5664\n",
      "Epoch [3/3], Step [13000/25252], Loss: 4067.5255\n",
      "Epoch [3/3], Step [13100/25252], Loss: 4548.4625\n",
      "Epoch [3/3], Step [13200/25252], Loss: 3569.5790\n",
      "Epoch [3/3], Step [13300/25252], Loss: 3834.3509\n",
      "Epoch [3/3], Step [13400/25252], Loss: 3618.4527\n",
      "Epoch [3/3], Step [13500/25252], Loss: 4053.7092\n",
      "Epoch [3/3], Step [13600/25252], Loss: 4412.0557\n",
      "Epoch [3/3], Step [13700/25252], Loss: 3181.4026\n",
      "Epoch [3/3], Step [13800/25252], Loss: 3621.0752\n",
      "Epoch [3/3], Step [13900/25252], Loss: 3257.1553\n",
      "Epoch [3/3], Step [14000/25252], Loss: 3209.2133\n",
      "Epoch [3/3], Step [14100/25252], Loss: 3055.3807\n",
      "Epoch [3/3], Step [14200/25252], Loss: 3556.9994\n",
      "Epoch [3/3], Step [14300/25252], Loss: 3889.9384\n",
      "Epoch [3/3], Step [14400/25252], Loss: 3049.3224\n",
      "Epoch [3/3], Step [14500/25252], Loss: 2927.7057\n",
      "Epoch [3/3], Step [14600/25252], Loss: 4372.0876\n",
      "Epoch [3/3], Step [14700/25252], Loss: 2999.2725\n",
      "Epoch [3/3], Step [14800/25252], Loss: 3822.5775\n",
      "Epoch [3/3], Step [14900/25252], Loss: 3286.2906\n",
      "Epoch [3/3], Step [15000/25252], Loss: 3355.0708\n",
      "Epoch [3/3], Step [15100/25252], Loss: 2780.7831\n",
      "Epoch [3/3], Step [15200/25252], Loss: 3998.4844\n",
      "Epoch [3/3], Step [15300/25252], Loss: 4509.8137\n",
      "Epoch [3/3], Step [15400/25252], Loss: 3292.8850\n",
      "Epoch [3/3], Step [15500/25252], Loss: 3320.0602\n",
      "Epoch [3/3], Step [15600/25252], Loss: 3152.1617\n",
      "Epoch [3/3], Step [15700/25252], Loss: 4900.5322\n",
      "Epoch [3/3], Step [15800/25252], Loss: 3128.5717\n",
      "Epoch [3/3], Step [15900/25252], Loss: 3854.8848\n",
      "Epoch [3/3], Step [16000/25252], Loss: 4686.0124\n",
      "Epoch [3/3], Step [16100/25252], Loss: 3531.1367\n",
      "Epoch [3/3], Step [16200/25252], Loss: 3569.6837\n",
      "Epoch [3/3], Step [16300/25252], Loss: 3867.0499\n",
      "Epoch [3/3], Step [16400/25252], Loss: 3067.0463\n",
      "Epoch [3/3], Step [16500/25252], Loss: 2804.5836\n",
      "Epoch [3/3], Step [16600/25252], Loss: 4780.2910\n",
      "Epoch [3/3], Step [16700/25252], Loss: 3498.4219\n",
      "Epoch [3/3], Step [16800/25252], Loss: 2792.4847\n",
      "Epoch [3/3], Step [16900/25252], Loss: 3405.3966\n",
      "Epoch [3/3], Step [17000/25252], Loss: 3565.8722\n",
      "Epoch [3/3], Step [17100/25252], Loss: 3233.2498\n",
      "Epoch [3/3], Step [17200/25252], Loss: 3138.0759\n",
      "Epoch [3/3], Step [17300/25252], Loss: 3586.6200\n",
      "Epoch [3/3], Step [17400/25252], Loss: 3939.8985\n",
      "Epoch [3/3], Step [17500/25252], Loss: 4061.9282\n",
      "Epoch [3/3], Step [17600/25252], Loss: 4046.1612\n",
      "Epoch [3/3], Step [17700/25252], Loss: 2967.6998\n",
      "Epoch [3/3], Step [17800/25252], Loss: 4319.5990\n",
      "Epoch [3/3], Step [17900/25252], Loss: 3454.8093\n",
      "Epoch [3/3], Step [18000/25252], Loss: 3603.0795\n",
      "Epoch [3/3], Step [18100/25252], Loss: 4188.2745\n",
      "Epoch [3/3], Step [18200/25252], Loss: 5285.9901\n",
      "Epoch [3/3], Step [18300/25252], Loss: 4207.2480\n",
      "Epoch [3/3], Step [18400/25252], Loss: 3471.3883\n",
      "Epoch [3/3], Step [18500/25252], Loss: 2913.4376\n",
      "Epoch [3/3], Step [18600/25252], Loss: 4281.5945\n",
      "Epoch [3/3], Step [18700/25252], Loss: 4259.8488\n",
      "Epoch [3/3], Step [18800/25252], Loss: 3906.8324\n",
      "Epoch [3/3], Step [18900/25252], Loss: 3359.5573\n",
      "Epoch [3/3], Step [19000/25252], Loss: 3952.1922\n",
      "Epoch [3/3], Step [19100/25252], Loss: 4061.0851\n",
      "Epoch [3/3], Step [19200/25252], Loss: 3604.5900\n",
      "Epoch [3/3], Step [19300/25252], Loss: 4159.6071\n",
      "Epoch [3/3], Step [19400/25252], Loss: 3297.2582\n",
      "Epoch [3/3], Step [19500/25252], Loss: 3876.5863\n",
      "Epoch [3/3], Step [19600/25252], Loss: 2925.6480\n",
      "Epoch [3/3], Step [19700/25252], Loss: 3995.9194\n",
      "Epoch [3/3], Step [19800/25252], Loss: 4141.5928\n",
      "Epoch [3/3], Step [19900/25252], Loss: 3257.0802\n",
      "Epoch [3/3], Step [20000/25252], Loss: 3619.6631\n",
      "Epoch [3/3], Step [20100/25252], Loss: 4777.8351\n",
      "Epoch [3/3], Step [20200/25252], Loss: 3764.9629\n",
      "Epoch [3/3], Step [20300/25252], Loss: 4278.7285\n",
      "Epoch [3/3], Step [20400/25252], Loss: 3142.3266\n",
      "Epoch [3/3], Step [20500/25252], Loss: 2582.7544\n",
      "Epoch [3/3], Step [20600/25252], Loss: 3675.7566\n",
      "Epoch [3/3], Step [20700/25252], Loss: 3627.6864\n",
      "Epoch [3/3], Step [20800/25252], Loss: 3520.8252\n",
      "Epoch [3/3], Step [20900/25252], Loss: 3814.7221\n",
      "Epoch [3/3], Step [21000/25252], Loss: 3521.7293\n",
      "Epoch [3/3], Step [21100/25252], Loss: 3982.9801\n",
      "Epoch [3/3], Step [21200/25252], Loss: 3707.8764\n",
      "Epoch [3/3], Step [21300/25252], Loss: 4215.5038\n",
      "Epoch [3/3], Step [21400/25252], Loss: 3622.1987\n",
      "Epoch [3/3], Step [21500/25252], Loss: 3896.3744\n",
      "Epoch [3/3], Step [21600/25252], Loss: 4822.4450\n",
      "Epoch [3/3], Step [21700/25252], Loss: 4425.2520\n",
      "Epoch [3/3], Step [21800/25252], Loss: 3622.2727\n",
      "Epoch [3/3], Step [21900/25252], Loss: 4301.6115\n",
      "Epoch [3/3], Step [22000/25252], Loss: 3936.3365\n",
      "Epoch [3/3], Step [22100/25252], Loss: 2963.6845\n",
      "Epoch [3/3], Step [22200/25252], Loss: 3009.4898\n",
      "Epoch [3/3], Step [22300/25252], Loss: 3410.5844\n",
      "Epoch [3/3], Step [22400/25252], Loss: 3906.9643\n",
      "Epoch [3/3], Step [22500/25252], Loss: 3457.6831\n",
      "Epoch [3/3], Step [22600/25252], Loss: 3516.6786\n",
      "Epoch [3/3], Step [22700/25252], Loss: 3819.8534\n",
      "Epoch [3/3], Step [22800/25252], Loss: 3176.4130\n",
      "Epoch [3/3], Step [22900/25252], Loss: 3177.2803\n",
      "Epoch [3/3], Step [23000/25252], Loss: 3504.3539\n",
      "Epoch [3/3], Step [23100/25252], Loss: 3665.1896\n",
      "Epoch [3/3], Step [23200/25252], Loss: 3203.2113\n",
      "Epoch [3/3], Step [23300/25252], Loss: 4272.1221\n",
      "Epoch [3/3], Step [23400/25252], Loss: 3979.6707\n",
      "Epoch [3/3], Step [23500/25252], Loss: 4235.6462\n",
      "Epoch [3/3], Step [23600/25252], Loss: 3992.1404\n",
      "Epoch [3/3], Step [23700/25252], Loss: 3755.4207\n",
      "Epoch [3/3], Step [23800/25252], Loss: 3470.0680\n",
      "Epoch [3/3], Step [23900/25252], Loss: 3505.9778\n",
      "Epoch [3/3], Step [24000/25252], Loss: 3375.7495\n",
      "Epoch [3/3], Step [24100/25252], Loss: 4645.5987\n",
      "Epoch [3/3], Step [24200/25252], Loss: 3453.2983\n",
      "Epoch [3/3], Step [24300/25252], Loss: 3117.0818\n",
      "Epoch [3/3], Step [24400/25252], Loss: 7075.5705\n",
      "Epoch [3/3], Step [24500/25252], Loss: 3720.3040\n",
      "Epoch [3/3], Step [24600/25252], Loss: 2755.9173\n",
      "Epoch [3/3], Step [24700/25252], Loss: 2962.5426\n",
      "Epoch [3/3], Step [24800/25252], Loss: 3211.5478\n",
      "Epoch [3/3], Step [24900/25252], Loss: 3358.7999\n",
      "Epoch [3/3], Step [25000/25252], Loss: 4118.3565\n",
      "Epoch [3/3], Step [25100/25252], Loss: 4496.6400\n",
      "Epoch [3/3], Step [25200/25252], Loss: 3126.6719\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 128\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Move model to device\n",
    "model_reg = model_reg.to(device)\n",
    "\n",
    "# Prepare DataLoader with padding for variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of (indices, score) tuples\n",
    "    indices_list, scores_list = zip(*batch)\n",
    "    lengths = [len(x) for x in indices_list]\n",
    "    max_len = max(lengths)\n",
    "    padded = [x + [0]*(max_len - len(x)) for x in indices_list]\n",
    "    padded_indices = torch.tensor(padded, dtype=torch.long)\n",
    "    scores = torch.tensor(scores_list, dtype=torch.float32)\n",
    "    return padded_indices, scores\n",
    "\n",
    "train_loader = DataLoader(title_indices_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Prepare target scores\n",
    "train_scores = torch.tensor(df_train['score_log'].values, dtype=torch.float32)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_reg.parameters(), lr=learning_rate)\n",
    "\n",
    "model_reg.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (batch_indices, batch_scores) in enumerate(train_loader):\n",
    "        batch_indices, batch_scores = batch_indices.to(device), batch_scores.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_reg(batch_indices)\n",
    "        loss = criterion(outputs, batch_scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_loss = running_loss / 100\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6109be79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151228fe279a4da890b0e88cfd6f738b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MSE: 5592.7798, Baseline MSE: 5998.0386\n",
      "Model MAE: 25.6618, Baseline MAE: 19.1959\n",
      "Model R2: 0.0084, Baseline R2: -0.0635\n"
     ]
    }
   ],
   "source": [
    "# Prepare test DataLoader\n",
    "test_loader = DataLoader(title_indices_test, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model_reg.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_indices, batch_scores in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch_indices = batch_indices.to(device)\n",
    "        outputs = model_reg(batch_indices).cpu().numpy()\n",
    "        all_preds.extend(outputs)\n",
    "        all_targets.extend(batch_scores.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "\n",
    "# Baseline: always predict average score from training set\n",
    "average_score = df_train['score_log'].mean()\n",
    "baseline_preds = np.full_like(all_targets, average_score)\n",
    "\n",
    "# Metrics\n",
    "mse_model = mean_squared_error(all_targets, all_preds)\n",
    "mse_baseline = mean_squared_error(all_targets, baseline_preds)\n",
    "\n",
    "r2_model = r2_score(all_targets, all_preds)\n",
    "r2_baseline = r2_score(all_targets, baseline_preds)\n",
    "\n",
    "mae_model = mean_absolute_error(all_targets, all_preds)\n",
    "mae_baseline = mean_absolute_error(all_targets, baseline_preds)\n",
    "\n",
    "print(f\"Model MSE: {mse_model:.4f}, Baseline MSE: {mse_baseline:.4f}\")\n",
    "print(f\"Model MAE: {mae_model:.4f}, Baseline MAE: {mae_baseline:.4f}\")\n",
    "print(f\"Model R2: {r2_model:.4f}, Baseline R2: {r2_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d7427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
