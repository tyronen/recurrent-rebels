{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90840573",
   "metadata": {},
   "source": [
    "# Continous Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35632570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f78372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e62fb6",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b308e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Dataset\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, token_indices, context_size):\n",
    "        self.data = token_indices\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 2*self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # context: C words before + C words after\n",
    "        context = (\n",
    "            self.data[idx : idx + self.context_size] +\n",
    "            self.data[idx + self.context_size + 1 : idx + 2*self.context_size + 1]\n",
    "        )\n",
    "        target = self.data[idx + self.context_size]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d99b67",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617a2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CBOW model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, 2*context_size)\n",
    "        embs = self.embeddings(inputs)      # → (batch_size, C, embed_dim)\n",
    "        embs = embs.mean(dim=1)             # → (batch_size, embed_dim)\n",
    "        out = self.linear(embs)             # → (batch_size, vocab_size)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd2984",
   "metadata": {},
   "source": [
    "# Load Text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97af6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text8 and build vocab, 50k is bigger than vocab size, if smaller than vocab size least frequent words replaced with unk\n",
    "def load_text8(path, max_vocab=50000):\n",
    "    with open(path, 'r') as f:\n",
    "        words = f.read().split()\n",
    "    # build most common vocab\n",
    "    freq = Counter(words)\n",
    "    most_common = freq.most_common(max_vocab-1)\n",
    "    idx_to_word = ['<unk>'] + [w for w,_ in most_common]\n",
    "    word_to_idx = {w:i for i,w in enumerate(idx_to_word)}\n",
    "    # map to indices (unk if not in vocab)\n",
    "    data = [word_to_idx.get(w, 0) for w in words]\n",
    "    return data, word_to_idx, idx_to_word\n",
    "\n",
    "data, w2i, i2w = load_text8(\"text8.txt\", max_vocab=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cfacc",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e413a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "def train(\n",
    "    data=data,\n",
    "    w2i=w2i, \n",
    "    i2w=i2w,\n",
    "    context_size=4,\n",
    "    embed_dim=128,\n",
    "    batch_size=256,\n",
    "    epochs=3,\n",
    "    lr=0.001,\n",
    "):\n",
    "\n",
    "    vocab_size = len(w2i)\n",
    "    dataset = CBOWDataset(data, context_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = CBOW(vocab_size, embed_dim).to(DEVICE)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(enumerate(dataloader, 1), total=len(dataloader), desc=f\"Epoch {epoch}\", unit=\"batch\", dynamic_ncols=True)\n",
    "\n",
    "        for step, (contexts, targets) in progress_bar:\n",
    "            contexts, targets = contexts.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(contexts)\n",
    "            loss = loss_fn(log_probs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / step\n",
    "            progress_bar.set_postfix(loss=avg_loss)\n",
    "\n",
    "        avg_epoch = total_loss / len(dataloader)\n",
    "\n",
    "        print(f\"✓ Epoch {epoch} complete. Average Loss: {avg_loss:.4f}\\n\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'cbow_text8_{timestamp}.pth'\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "    # save\n",
    "    torch.save({\n",
    "        'model_state': model.state_dict(),\n",
    "        'word_to_idx': w2i,\n",
    "        'idx_to_word': i2w,\n",
    "        'embed_dim': embed_dim,\n",
    "        'context_size': context_size\n",
    "    }, filepath)\n",
    "    print(f\"Model saved {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d61b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 87/66427 [02:07<26:59:15,  1.46s/batch, loss=10.3]\n",
      "Epoch 1: 100%|██████████| 66427/66427 [10:23<00:00, 106.56batch/s, loss=6.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Epoch 1 complete. Average Loss: 6.3487\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/66427 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98945dec",
   "metadata": {},
   "source": [
    "# Load Model after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = Path(filepath)\n",
    "ckpt = torch.load(ckpt_path, map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb203e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-instantiate model and load weights\n",
    "vocab_size = len(ckpt[\"idx_to_word\"])\n",
    "embed_dim  = ckpt[\"embed_dim\"]\n",
    "window_sz  = ckpt[\"context_size\"]\n",
    "\n",
    "model = CBOW(vocab_size, embed_dim).to(DEVICE).eval()\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "# Helper dicts\n",
    "idx2word = ckpt[\"idx_to_word\"]\n",
    "word2idx = ckpt[\"word_to_idx\"]\n",
    "\n",
    "embedding_matrix = model.embeddings.weight.detach()   # (V, D)\n",
    "embedding_matrix = F.normalize(embedding_matrix, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638c7a0",
   "metadata": {},
   "source": [
    "# Find top K similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbours(query_word, k=5):\n",
    "    q_idx = word2idx.get(query_word, 0)\n",
    "    q_vec = embedding_matrix[q_idx]                 # (D,)\n",
    "    cos = torch.mv(embedding_matrix, q_vec)         # cosine similarity to all vocab\n",
    "    topk = cos.topk(k+1).indices.tolist()           # +1 because first will be the word itself\n",
    "    topk = [i for i in topk if i != q_idx][:k]\n",
    "    return [idx2word[i] for i in topk]\n",
    "\n",
    "print(nearest_neighbours(\"queen\", k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3f39b",
   "metadata": {},
   "source": [
    "# Evals\n",
    "By Tyrone on Gitub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927b8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_function import word_analogy_test, semantic_similarity_test, category_clustering_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acdab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_embeddings(embedding_matrix, word2idx):\n",
    "    # Run all tests\n",
    "    analogy_hard_score, analogy_soft_score = word_analogy_test(embedding_matrix, word2idx)\n",
    "    similarity_score, dissimilarity_score = semantic_similarity_test(embedding_matrix, word2idx)\n",
    "    clustering_score = category_clustering_test(embedding_matrix, word2idx)\n",
    "\n",
    "    overall_score = (analogy_soft_score + similarity_score + clustering_score) / 3\n",
    "\n",
    "    return overall_score, analogy_soft_score, similarity_score, dissimilarity_score, clustering_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_embeddings(embedding_matrix, word2idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
